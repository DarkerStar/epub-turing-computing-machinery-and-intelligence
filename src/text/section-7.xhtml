<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en" xml:lang="en">
<head>
	<meta charset="utf-8"/>
	
	<title>7. Learning Machines • Computing Machinery and Intelligence</title>
	
	<link rel="stylesheet" href="../style/style.css"/>
</head>
<body epub:type="bodymatter">
<section id="section-7" epub:type="chapter">
<h1 id="learning-machines"><span class="section-number">7.</span> Learning Machines</h1>

<p>
	The reader will have anticipated that I have no very convincing arguments of a positive nature to support my views.
	If I had I should not have taken such pains to point out the fallacies in contrary views.
	Such evidence as I have I shall now give.
</p>
<p>
	Let us return for a moment to Lady Lovelace’s objection, which stated that the machine can only do what we tell it to do.
	One could say that a man can ‘inject’ an idea into the machine, and that it will respond to a certain extent and then drop into quiescence, like a piano string struck by a hammer.
	Another simile would be an atomic pile of less than critical size: an injected idea is to correspond to a neutron entering the pile from without.
	Each such neutron will cause a certain disturbance which eventually dies away.
	If, however, the size of the pile is sufficiently increased, the disturbance caused by such an incoming neutron will very likely go on and on increasing until the whole pile is destroyed.
	Is there a corresponding phenomenon for minds, and is there one for machines?
	There does seem to be one for the human mind.
	The majority of them seem to be ‘sub-critical’, <abbr title="that is"><i lang="la" xml:lang="la">i.e.</i></abbr> to correspond in this analogy to piles of sub-critical size.
	An idea presented to such a mind will on average give rise to less than one idea in reply.
	A smallish proportion are super-critical.
	An idea presented to such a mind that may give rise to a whole ‘theory’ consisting of secondary, tertiary and more remote ideas.
	Animals minds seem to be very definitely sub-critical.
	Adhering to this analogy we ask, “Can a machine be made to be super-critical?”
</p>
<p>
	The ‘skin-of-an-onion’ analogy is also helpful.
	In considering the functions of the mind or the brain we find certain operations which we can explain in purely mechanical terms.
	This we say does not correspond to the real mind: it is a sort of skin which we must strip off if we are to find the real mind.
	But then in what remains we find a further skin to be stripped off, and so on.
	<span epub:type="pagebreak" id="page-455" title="455"></span>
	Proceeding in this way do we ever come to the ‘real’ mind, or do we eventually come to the skin which has nothing in it?
	In the latter case the whole mind is mechanical.
	(It would not be a discrete-state machine however.
	We have discussed this.)
</p>
<p>
	These last two paragraphs do not claim to be convincing arguments.
	They should rather be described as “recitations tending to produce belief”.
</p>
<p>
	The only really satisfactory support that can be given for the view expressed at the beginning of <a href="section-6.xhtml">§6</a>, will be that provided by waiting for the end of the century and then doing the experiment described.
	But what can we say in the meantime?
	What steps should be taken now if the experiment is to be successful?
</p>
<p>
	As I have explained, the problem is mainly one of programming.
	Advances in engineering will have to be made too, but it seems unlikely that these will not be adequate for the requirements.
	Estimates of the storage capacity of the brain vary from 10<sup>10</sup> to 10<sup>15</sup> binary digits.
	I incline to the lower values and believe that only a very small fraction is used for the higher types of thinking.
	Most of it is probably used for the retention of visual impressions, I should be surprised if more than 10<sup>9</sup> was required for satisfactory playing of the imitation game, at any rate against a blind man.
	(Note—The capacity of the <i>Encyclopaedia Britannica</i>, 11th edition, is <span class="nobr">2 × 10<sup>9</sup></span>.)
	A storage capacity of 10<sup>7</sup>, would be a very practicable possibility even by present techniques.
	It is probably not necessary to increase the speed of operations of the machines at all.
	Parts of modern machines which can be regarded as analogs of nerve cells work about a thousand times faster than the latter.
	This should provide a ‘margin of safety’ which could cover losses of speed arising in many ways, Our problem then is to find out how to programme these machines to play the game.
	At my present rate of working I produce about a thousand digits of programme a day, so that about sixty workers, working steadily through the fifty years might accomplish the job, if nothing went into the wastepaper basket.
	Some more expeditious method seems desirable.
</p>
<p>
	In the process of trying to imitate an adult human mind we are bound to think a good deal about the process which has brought it to the state that it is in.
	We may notice three components.
</p>
<ol class="list-lower-alpha">
	<li>The initial state of the mind, say at birth,</li>
	<li>The education to which it has been subjected,</li>
	<li>Other experience, not to be described as education, to which it has been subjected.</li>
</ol>
<p>
	<span epub:type="pagebreak" id="page-456" title="456"></span>
	Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child’s?
	If this were then subjected to an appropriate course of education one would obtain the adult brain.
	Presumably the child brain is something like a note-book as one buys it from the stationer’s.
	Rather little mechanism, and lots of blank sheets.
	(Mechanism and writing are from our point of view almost synonymous.)
	Our hope is that there is so little mechanism in the child brain that something like it can be easily programmed.
	The amount of work in the education we can assume, as a first approximation, to be much the same as for the human child.
</p>
<p>
	We have thus divided our problem into two parts.
	The child programme and the education process.
	These two remain very closely connected.
	We cannot expect to find a good child-machine at the first attempt.
	One must experiment with teaching one such machine and see how well it learns.
	One can then try another and see if it is better or worse.
	There is an obvious connection between this process and evolution, by the identifications
</p>
<ul class="list-table">
	<li>
		<span>Structure of the child machine</span>
		<span>=</span>
		<span>Hereditary material</span>
	</li>
	<li>
		<span>
			<span class="ditto-cell">
				<span>Changes</span>
				<span>„</span>
				<span>„</span>
			</span>
		</span>
		<span>=</span>
		<span>Mutations</span>
	</li>
	<li>
		<span>Natural selection</span>
		<span>=</span>
		<span>Judgment of the experimenter</span>
	</li>
</ul>
<p class="no-indent">
	One may hope, however, that this process will be more expeditious than evolution.
	The survival of the fittest is a slow method for measuring advantages.
	The experimenter, by the exercise of intelligence, should he able to speed it up.
	Equally important is the fact that he is not restricted to random mutations.
	If he can trace a cause for some weakness he can probably think of the kind of mutation which will improve it.
</p>
<p>
	It will not be possible to apply exactly the same teaching process to the machine as to a normal child.
	It will not, for instance, be provided with legs, so that it could not be asked to go out and fill the coal scuttle.
	Possibly it might not have eyes.
	But however well these deficiencies might be overcome by clever engineering, one could not send the creature to school without the other children making excessive fun of it.
	It must be given some tuition.
	We need not be too concerned about the legs, eyes, <abbr title="and so on.">etc.</abbr>
	The example of Miss <cite>Helen Keller</cite> shows that education can take place provided that communication in both directions between teacher and pupil can take place by some means or other.
</p>
<p>
	<span epub:type="pagebreak" id="page-457" title="457"></span>
	We normally associate punishments and rewards with the teaching process.
	Some simple child-machines can be constructed or programmed on this sort of principle.
	The machine has to be so constructed that events which shortly preceded the occurrence of a punishment-signal are unlikely to be repeated, whereas a reward-signal increased the probability of repetition of the events which led up to it.
	These definitions do not presuppose any feelings on the part of the machine.
	I have done some experiments with one such child-machine, and succeeded in teaching it a few things, but the teaching method was too unorthodox for the experiment to be considered really successful.
</p>
<p>
	The use of punishments and rewards can at best be a part of the teaching process.
	Roughly speaking, if the teacher has no other means of communicating to the pupil, the amount of information which can reach him does not exceed the total number of rewards and punishments applied.
	By the time a child has learnt to repeat ‘Casabianca’ he would probably feel very sore indeed, if the text could only be discovered by a ‘Twenty Questions’ technique, every “NO” taking the form of a blow.
	It is necessary therefore to have some other ‘unemotional’ channels of communication.
	If these are available it is possible to teach a machine by punishments and rewards to obey orders given in some language, <abbr title="for example"><i lang="la" xml:lang="la">e.g.</i></abbr> a symbolic language.
	These orders are to be transmitted through the ‘unemotional’ channels.
	The use of this language will diminish greatly the number of punishments and rewards required.
</p>
<p>
	Opinions may vary as to the complexity which is suitable in the child machine.
	One might try to make it as simple as possible consistently with the general principles.
	Alternatively one might have a complete system of logical inference ‘built in’.<sup><a id="fn3-src" epub:type="noteref" href="#fn3" class="footnote-ref">3</a></sup>
	In the latter case the store would be largely occupied with definitions and propositions.
	The propositions would have various kinds of status, <abbr title="for example"><i lang="la" xml:lang="la">e.g.</i></abbr> well-established facts, conjectures, mathematically proved theorems, statements given by an authority, expressions having the logical form of proposition but not belief-value.
	Certain propositions may be described as ‘imperatives’.
	The machine should be so constructed that as soon as an imperative is classed as ‘well-established’ the appropriate action automatically takes place.
	To illustrate this, suppose the teacher says to the machine, “Do your homework now”.
	This may cause “Teacher says ‘Do your homework now’” to be included amongst the well-established facts.
	Another such fact might be,<span epub:type="pagebreak" id="page-458" title="458"></span> “Everything that teacher says is true”.
	Combining these may eventually lead to the imperative, “Do your homework now”, being included amongst the well-established facts, and this, by the construction of the machine, will mean that the homework actually gets started, but the effect is very satisfactory.
	The processes of inference used by the machine need not be such as would satisfy the most exacting logicians.
	There might for instance be no hierarchy of types.
	But this need not mean that type fallacies will occur, any more than we are bound to fall over unfenced cliffs.
	Suitable imperatives (expressed <em>within</em> the systems, not forming part of the rules <em>of</em> the system) such as ‘Do not use a class unless it is a subclass of one which has been mentioned by teacher’ can have a similar effect to ‘Do not go too near the edge’.
</p>
<aside id="fn3" epub:type="footnote" class="footnote">
	<a class="footnote-return" href="#fn3-src"><sup>3</sup></a>
	Or rather ‘programmed in’ for our child-machine will be programmed in a digital computer. But the logical system will not have to be learnt.
</aside>
<p>
	The imperatives that can be obeyed by a machine that has no limbs are bound to be of a rather intellectual character, as in the example (doing homework) given above.
	Important amongst such imperatives will be ones which regulate the order in which the rules of the logical system concerned are to be applied.
	For at each stage when one is using a logical system, there is a very large number of alternative steps, any of which one is permitted to apply, so far as obedience to the rules of the logical system is concerned.
	These choices make the difference between a brilliant and a footling reasoner, not the difference between a sound and a fallacious one.
	Propositions leading to imperatives of this kind might be “When Socrates is mentioned, use the syllogism in Barbara” or “If one method has been proved to be quicker than another, do not use the slower method”.
	Some of these may be ‘given by authority’, but others may be produced by the machine itself, <abbr title="for example"><i lang="la" xml:lang="la">e.g.</i></abbr> by scientific induction.
</p>
<p>
	The idea of a learning machine may appear paradoxical to some readers.
	How can the rules of operation of the machine change?
	They should describe completely how the machine will react whatever its history might be, whatever changes it might undergo.
	The rules are thus quite time-invariant.
	This is quite true.
	The explanation of the paradox is that the rules which get changed in the learning process are of a rather less pretentious kind, claiming only an ephemeral validity.
	The reader may draw a parallel with the Constitution of the United States.
</p>
<p>
	An important feature of a learning machine is that its teacher will often be very largely ignorant of quite what is going on inside, although he may still be able to some extent to predict his pupil’s behavior.
	This should apply most strongly to the<span epub:type="pagebreak" id="page-459" title="459"></span> later education of a machine arising from a child-machine of well-tried design (or programme).
	This is in clear contrast with normal procedure when using a machine to do computations one’s object is then to have a clear mental picture of the state of the machine at each moment in the computation.
	This object can only be achieved with a struggle.
	The view that “the machine can only do what we know how to order it to do”,<sup><a id="fn4-src" epub:type="noteref" href="#fn4" class="footnote-ref">4</a></sup> appears strange in face of this.
	Most of the programmes which we can put into the machine will result in its doing something that we cannot make sense of at all, or which we regard as completely random behaviour.
	Intelligent behaviour presumably consists in a departure from the completely disciplined behaviour involved in computation, but a rather slight one, which does not give rise to random behaviour, or to pointless repetitive loops.
	Another important result of preparing our machine for its part in the imitation game by a process of teaching and learning is that ‘human fallibility’ is likely to be omitted in a rather natural way, <abbr title="that is"><i lang="la" xml:lang="la">i.e.</i></abbr> without special ‘coaching’.
	(The reader should reconcile this with the point of view on <abbr title="pages">pp.</abbr> 24, 25.)
	Processes that are learnt do not produce a hundred per cent. certainty of result; if they did they could not be unlearnt.
</p>
<aside id="fn4" epub:type="footnote" class="footnote">
	<a class="footnote-return" href="#fn4-src"><sup>4</sup></a>
	Compare Lady Lovelace’s statement (<a href="section-6.xhtml#page-450" class="page-link"><abbr title="page">p.</abbr> 450</a>), which does not contain the word ‘only’.
</aside>
<p>
	It is probably wise to include a random element in a learning machine (see <a href="section-4.xhtml#page-438" class="page-link"><abbr title="page">p.</abbr> 438</a>).
	A random element is rather useful when we are searching for a solution of some problem.
	Suppose for instance we wanted to find a number between 50 and 200 which was equal to the square of the sum of its digits, we might start at 51 then try 52 and go on until we got a number that worked.
	Alternatively we might choose numbers at random until we got a good one.
	This method has the advantage that it is unnecessary to keep track of the values that have been tried, but the disadvantage that one may try the same one twice, but this is not very important if there are several solutions.
	The systematic method has the disadvantage that there may be an enormous block without any solutions in the region which has to be investigated first.
	Now the learning process may be regarded as a search for a form of behaviour which will satisfy the teacher (or some other criterion).
	Since there is probably a very large number of satisfactory solutions the random method seems to be better than the systematic.
	It should be noticed that it is used in the analogous process of evolution.
	But there the systematic method is not possible.
	How could one keep track of the<span epub:type="pagebreak" id="page-460" title="460"></span> different genetical combinations that had been tried, so as to avoid trying them again?
</p>
<p>
	We may hope that machines will eventually compete with men in all purely intellectual fields.
	But which are the best ones to start with?
	Even this is a difficult decision.
	Many people think that a very abstract activity, like the playing of chess, would be best.
	It can also be maintained that it is best to provide the machine with the best sense organs that money can buy, and then teach it to understand and speak English.
	This process could follow the normal teaching of a child.
	Things would be pointed out and named, <abbr title="and so on.">etc.</abbr>
	Again I do not know what the right answer is, but I think both approaches should be tried.
</p>
<p>
	We can only see a short distance ahead, but we can see plenty there that needs to be done.
</p>
</section>
</body>
</html>
